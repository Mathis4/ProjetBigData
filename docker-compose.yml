version: "3.8"

services:
  zookeeper:
    image: wurstmeister/zookeeper
    ports:
      - "2181:2181"

  elasticsearch:
    image: elasticsearch:8.15.3
    ports:
      - "9200:9200"
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9200/_cluster/health" ]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: "2.0"
        reservations:
          memory: 1g
          cpus: "1.0"

  kibana:
    image: kibana:8.15.3
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    ports:
      - "5601:5601"
    depends_on:
      elasticsearch:
        condition: service_healthy

  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.0.1
    environment:
      - MLFLOW_HOME=/mlflow
      - MLFLOW_TRACKING_URI=sqlite:////mlflow/mlruns.db
    ports:
      - "5000:5000"
    command: ["mlflow", "ui", "--host", "0.0.0.0", "--port", "5000"]
    volumes:
      - ./models/mlartifacts:/mlartifacts/0
      - ./models/mlruns:/mlruns/0

  kafka:
    image: wurstmeister/kafka
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      - KAFKA_ADVERTISED_HOST_NAME=kafka
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
    depends_on:
      zookeeper:
        condition: service_started
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9092"]
      interval: 30s
      timeout: 10s
      retries: 5

  spark-master:
    image: bitnami/spark:3.3.1
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
    ports:
      - "8080:8080"
      - "7077:7077"

  spark-worker:
    image: bitnami/spark:3.3.1
    container_name: spark-worker
    depends_on:
      spark-master:
        condition: service_started
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
    ports:
      - "8081:8081"

  python-app:
    build:
      context: data_ingestion/spark_collector
      dockerfile: Dockerfile
    container_name: python-app
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_PACKAGES=org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1
      - PYSPARK_SUBMIT_ARGS=--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.elasticsearch:elasticsearch-spark-30_2.12:8.7.0 pyspark-shell
    depends_on:
      elasticsearch:
        condition: service_healthy
      kafka:
        condition: service_healthy
      spark-master:
        condition: service_started
      spark-worker:
        condition: service_started
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: "1.5"
        reservations:
          memory: 1g
          cpus: "1.0"

  github_collector:
    build:
      context: data_ingestion/github_collector
      dockerfile: Dockerfile
    container_name: github
    env_file:
      - .env
    depends_on:
      elasticsearch:
        condition: service_healthy
      kafka:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: "1.0"
        reservations:
          memory: 1g
          cpus: "0.5"

  train-part:
    build:
      context: train_model
      dockerfile: Dockerfile
    container_name: train-part
    env_file:
      - .env

  fastapi:
    build:
      context: fastapi
      dockerfile: Dockerfile
    container_name: fastapi_backend
    ports:
      - "8000:8000"
    depends_on:
      python-app:
        condition: service_started

  gradio:
    build:
      context: gradio
      dockerfile: Dockerfile
    container_name: gradio_interface
    ports:
      - "7860:7860"
    depends_on:
      fastapi:
        condition: service_started
