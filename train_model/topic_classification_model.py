import os
import threading
import time
import mlflow
import mlflow.keras
import numpy as np
import pandas as pd
from collections import Counter
from elasticsearch import Elasticsearch
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense
from mlflow.models import ModelSignature
from mlflow.types import Schema, TensorSpec


def train_model():
    """Fonction d'entraÃ®nement du modÃ¨le."""

    # Ã‰tape 1 : RÃ©cupÃ©rer les donnÃ©es depuis Elasticsearch
    print("RÃ©cupÃ©ration des donnÃ©es depuis Elasticsearch...")
    es = Elasticsearch("http://elasticsearch:9200")
    index_name = "github_repos"
    query = {"query": {"match_all": {}}}
    response = es.search(index=index_name, body=query, size=9999)
    data = [hit["_source"] for hit in response["hits"]["hits"]]

    print(f"Nombre de documents rÃ©cupÃ©rÃ©s: {len(data)}")

    # Ã‰tape 2 : PrÃ©parer les donnÃ©es
    print("PrÃ©paration des donnÃ©es...")
    df = pd.DataFrame(data)

    # Filtrer les lignes oÃ¹ la colonne "topics" est vide
    df = df[df["topics"].apply(lambda x: len(x) > 0)]

    print(f"Nombre de lignes aprÃ¨s filtrage: {len(df)}")

    # GÃ©nÃ©rer la liste des labels uniques Ã  partir des topics
    all_labels = [label for topics in df["topics"] for label in topics]
    label_counts = Counter(all_labels)

    # Afficher les 50 labels les plus frÃ©quents
    most_common_labels = label_counts.most_common(50)
    print(f"50 labels les plus frÃ©quents: {most_common_labels}")

    # SÃ©lectionner les 50 labels les plus frÃ©quents
    labels_to_keep = [label for label, count in most_common_labels]

    print(f"Nombre de labels conservÃ©s: {len(labels_to_keep)}")

    # CrÃ©er une colonne binaire pour chaque label
    for label in labels_to_keep:
        df[label] = df["topics"].apply(lambda topics: 1 if label in topics else 0)

    # Fonction pour vÃ©rifier et aplatir les embeddings
    def flatten_embedding(embedding):
        if isinstance(embedding, list) and len(embedding) > 0 and isinstance(embedding[0], list):
            # Aplatir une liste de listes
            return [item for sublist in embedding for item in sublist]
        elif isinstance(embedding, list):
            return embedding
        else:
            return []

    # Appliquer le flattening aux colonnes d'embeddings
    print("Aplatissement des embeddings...")
    df["description_embedding"] = df["description_embedding"].apply(flatten_embedding)
    df["languages_embedding"] = df["languages_embedding"].apply(flatten_embedding)
    df["repo_name_embedding"] = df["repo_name_embedding"].apply(flatten_embedding)

    # DÃ©terminer la longueur maximale des embeddings
    embedding_length = max(
        df["description_embedding"].apply(len).max(),
        df["languages_embedding"].apply(len).max(),
        df["repo_name_embedding"].apply(len).max()
    )

    print(f"Longueur maximale des embeddings: {embedding_length}")

    # Fonction de padding pour uniformiser la taille des embeddings
    def pad_embedding(embedding, length=embedding_length):
        return embedding + [0] * (length - len(embedding))

    df["description_embedding"] = df["description_embedding"].apply(lambda x: pad_embedding(x, embedding_length))
    df["languages_embedding"] = df["languages_embedding"].apply(lambda x: pad_embedding(x, embedding_length))
    df["repo_name_embedding"] = df["repo_name_embedding"].apply(lambda x: pad_embedding(x, embedding_length))

    # Pour construire X, on va concatÃ©ner les trois embeddings pour chaque exemple
    X_desc = np.array(df["description_embedding"].tolist(), dtype=np.float32)
    X_lang = np.array(df["languages_embedding"].tolist(), dtype=np.float32)
    X_repo = np.array(df["repo_name_embedding"].tolist(), dtype=np.float32)

    # Concatenation horizontale : chaque exemple aura une taille = embedding_length * 3
    X = np.concatenate([X_desc, X_lang, X_repo], axis=1)

    # y : matrice binaire des labels
    y = df[labels_to_keep].values.astype(np.float32)

    # Ã‰tape 3 : Diviser les donnÃ©es en ensembles d'entraÃ®nement et de test
    print("Division des donnÃ©es en ensembles d'entraÃ®nement et de test...")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # DÃ©marrer une nouvelle expÃ©rience avec MLflow
    mlflow.set_tracking_uri("http://mlflow:5000")  # Pointage vers l'instance MLflow
    mlflow.keras.autolog()  # Active l'autologging pour Keras
    with mlflow.start_run():  # DÃ©marrage d'une nouvelle run

        print("CrÃ©ation du modÃ¨le...")
        # Ã‰tape 4 : CrÃ©er le modÃ¨le
        model = Sequential([
            Dense(64, activation='relu'),
            Dense(32, activation='relu'),
            Dense(16, activation='relu'),
            Dense(len(labels_to_keep), activation='sigmoid')
        ])
        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

        print("Enregistrement des paramÃ¨tres dans MLflow...")
        # Enregistrement des paramÃ¨tres du modÃ¨le dans MLflow
        mlflow.autolog()

        artifact_file = "labels.json"
        labels_dict = {"labels": labels_to_keep}
        mlflow.log_dict(labels_dict, artifact_file)

        print("EntraÃ®nement du modÃ¨le...")
        # Ã‰tape 5 : EntraÃ®ner le modÃ¨le
        history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))

        print("Ã‰valuation du modÃ¨le...")
        # Ã‰tape 6 : Ã‰valuer le modÃ¨le
        loss, accuracy = model.evaluate(X_test, y_test)
        print(f"Loss: {loss}, Accuracy: {accuracy}")

        print("Sauvegarde du modÃ¨le dans MLflow...")
        # Ã‰tape 7 : Sauvegarder le modÃ¨le dans MLflow avec signature et exemple d'entrÃ©e
        input_example = X_train[0:1].astype(np.float32)  # Exemple d'entrÃ©e

        # DÃ©finir la signature manuellement en utilisant TensorSpec
        input_schema = Schema([TensorSpec(type=np.dtype(np.float32), shape=(-1, X.shape[1]))])
        output_schema = Schema([TensorSpec(type=np.dtype(np.float32), shape=(-1, len(labels_to_keep)))])
        signature = ModelSignature(inputs=input_schema, outputs=output_schema)

        mlflow.keras.log_model(model, "model", signature=signature)

        print("ModÃ¨le enregistrÃ©.")


def get_dataset_from_elasticsearch():
    """RÃ©cupÃ¨re les donnÃ©es depuis Elasticsearch."""
    print("RÃ©cupÃ©ration des donnÃ©es depuis Elasticsearch...")
    es = Elasticsearch("http://elasticsearch:9200")

    index_name = "github_repos"
    query = {"query": {"match_all": {}}}
    response = es.search(index=index_name, body=query, size=1000)

    data = [hit["_source"] for hit in response["hits"]["hits"]]
    print(f"Nombre de documents rÃ©cupÃ©rÃ©s: {len(data)}")

    return data


class DatasetScheduler:
    def __init__(self, interval=600):
        """
        interval : temps en secondes entre chaque vÃ©rification (600s = 10 minutes)
        """
        self.interval = interval
        self.previous_data_count = None
        self.running = True

    def get_dataset_from_elasticsearch(self):
        """RÃ©cupÃ©rer le dataset depuis Elasticsearch."""
        try:
            es = Elasticsearch("http://elasticsearch:9200")
            if not es.ping():
                print("âš ï¸ Impossible de se connecter Ã  Elasticsearch.")
                return []

            print("ğŸ” Connexion Ã  Elasticsearch rÃ©ussie.")
            index_name = "github_repos"
            query = {"query": {"match_all": {}}}
            response = es.search(index=index_name, body=query, size=1000)
            data = [hit["_source"] for hit in response["hits"]["hits"]]
            return data
        except Exception as e:
            print(f"âš ï¸ Erreur lors de la connexion Ã  Elasticsearch: {e}")
            return []

    def check_and_train(self):
        """VÃ©rifie le dataset et dÃ©clenche l'entraÃ®nement si nÃ©cessaire."""
        while self.running:
            print("\nğŸ” VÃ©rification du dataset dans Elasticsearch...")
            data = self.get_dataset_from_elasticsearch()
            current_data_count = len(data)

            if not data:
                print("âš ï¸ Dataset vide ou impossible de rÃ©cupÃ©rer les donnÃ©es.")
            else:
                try:
                    es = Elasticsearch("http://elasticsearch:9200")
                    meta_index = "training_metadata"

                    # VÃ©rifier s'il existe une valeur enregistrÃ©e
                    try:
                        last_record = es.get(index=meta_index, id=1)
                        stored_data_count = last_record["_source"]["data_count"]
                    except Exception:
                        stored_data_count = None

                    print(f"ğŸ“Š DerniÃ¨re valeur enregistrÃ©e: {stored_data_count}")
                    print(f"ğŸ“ˆ Nombre actuel de documents: {current_data_count}")

                    # ğŸš€ Toujours dÃ©clencher l'entraÃ®nement si on a au moins 400 documents
                    if current_data_count >= 400:
                        print("ğŸš€ Dataset de taille suffisante, lancement de l'entraÃ®nement !")
                        train_model()  # Appel Ã  la fonction d'entraÃ®nement

                        # Mise Ã  jour du nombre de documents dans Elasticsearch
                        es.index(index=meta_index, id=1, body={"data_count": current_data_count})
                    else:
                        print("âœ… Dataset insuffisant, attente de la prochaine vÃ©rification.")

                except Exception as e:
                    print(f"âš ï¸ Erreur avec Elasticsearch: {e}")

            time.sleep(self.interval)

    def start(self):
        """DÃ©marrer le scheduler dans un thread sÃ©parÃ©."""
        print("â³ DÃ©marrage du scheduler pour surveiller Elasticsearch...")
        thread = threading.Thread(target=self.check_and_train, daemon=True)
        thread.start()


if __name__ == "__main__":
    scheduler = DatasetScheduler(interval=int(os.getenv("MODEL_TRAINING_INTERVAL")))  # VÃ©rifie toutes les 10 minutes
    scheduler.start()

    while True:
        time.sleep(3600)  # EmpÃªche le script de se terminer
